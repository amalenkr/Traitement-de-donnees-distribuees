{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
    "nltk.download('punkt')\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from tqdm import tqdm\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré process des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amale\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_pol = pd.read_csv('1- données/Political-media-DFE.csv', encoding='latin-1')\n",
    "df_eco = pd.read_csv('1- données/Full-Economic-News-DFE-839861.csv', encoding='latin-1')\n",
    "df_movies = pd.read_csv('1- données/movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @nowthisnews: Rep. Trey Radel (R- #FL) slams #Obamacare. #politics https://t.co/zvywMG8yIH'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pol.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW YORK -- Yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week, reflecting the overall decline in short-term interest rates.</br></br>On small-denomination, or \"consumer,\" CDs sold directly by banks, the average yield on six-month deposits fell to 5.49% from 5.62% in the week ended yesterday, according to an 18-bank survey by Banxquote Money Markets, a Wilmington, Del., information service.</br></br>On three-month \"consumer\" deposits, the average yield sank to 5.29% from 5.42% the week before, according to Banxquote. Two banks in the Banxquote survey, Citibank in New York and CoreStates in Pennsylvania, are paying less than 5% on threemonth small-denomination CDs.</br></br>Declines were somewhat smaller on five-year consumer CDs, which eased to 7.37% from 7.45%, Banxquote said.</br></br>Yields on three-month and six-month Treasury bills sold at Monday\\'s auction plummeted more than a fifth of a percentage point from the previous week, to 5.46% and 5.63%, respectively.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eco.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.overview.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(tokens):\n",
    "  tokens = map(lambda x: x.replace('#', ''), tokens)\n",
    "  return list(tokens)\n",
    "\n",
    "def remove_url(tokens):\n",
    "  tokens = filter(lambda x: \"http\" not in x, tokens)\n",
    "  return list(tokens)\n",
    "\n",
    "def remove_html(tokens):\n",
    "  tokens = filter(lambda x: x[0]+x[-1] != '<>', tokens)\n",
    "  return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@nowthisnews',\n",
       " ':',\n",
       " 'Rep',\n",
       " '.',\n",
       " 'Trey',\n",
       " 'Radel',\n",
       " '(',\n",
       " 'R',\n",
       " '-',\n",
       " 'FL',\n",
       " ')',\n",
       " 'slams',\n",
       " 'Obamacare',\n",
       " '.',\n",
       " 'politics',\n",
       " 'https://t.co/zvywMG8yIH']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_hashtags(TweetTokenizer().tokenize(df_pol.text.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@nowthisnews',\n",
       " ':',\n",
       " 'Rep',\n",
       " '.',\n",
       " 'Trey',\n",
       " 'Radel',\n",
       " '(',\n",
       " 'R',\n",
       " '-',\n",
       " '#FL',\n",
       " ')',\n",
       " 'slams',\n",
       " '#Obamacare',\n",
       " '.',\n",
       " '#politics']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(TweetTokenizer().tokenize(df_pol.text.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEW',\n",
       " 'YORK',\n",
       " '-',\n",
       " '-',\n",
       " 'Yields',\n",
       " 'on',\n",
       " 'most',\n",
       " 'certificates',\n",
       " 'of',\n",
       " 'deposit',\n",
       " 'offered',\n",
       " 'by',\n",
       " 'major',\n",
       " 'banks',\n",
       " 'dropped',\n",
       " 'more',\n",
       " 'than',\n",
       " 'a',\n",
       " 'tenth',\n",
       " 'of',\n",
       " 'a',\n",
       " 'percentage',\n",
       " 'point',\n",
       " 'in',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'week',\n",
       " ',',\n",
       " 'reflecting',\n",
       " 'the',\n",
       " 'overall',\n",
       " 'decline',\n",
       " 'in',\n",
       " 'short-term',\n",
       " 'interest',\n",
       " 'rates',\n",
       " '.',\n",
       " 'On',\n",
       " 'small-denomination',\n",
       " ',',\n",
       " 'or',\n",
       " '\"',\n",
       " 'consumer',\n",
       " ',',\n",
       " '\"',\n",
       " 'CDs',\n",
       " 'sold',\n",
       " 'directly',\n",
       " 'by',\n",
       " 'banks',\n",
       " ',',\n",
       " 'the',\n",
       " 'average',\n",
       " 'yield',\n",
       " 'on',\n",
       " 'six-month',\n",
       " 'deposits',\n",
       " 'fell',\n",
       " 'to',\n",
       " '5.49',\n",
       " '%',\n",
       " 'from',\n",
       " '5.62',\n",
       " '%',\n",
       " 'in',\n",
       " 'the',\n",
       " 'week',\n",
       " 'ended',\n",
       " 'yesterday',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'an',\n",
       " '18',\n",
       " '-',\n",
       " 'bank',\n",
       " 'survey',\n",
       " 'by',\n",
       " 'Banxquote',\n",
       " 'Money',\n",
       " 'Markets',\n",
       " ',',\n",
       " 'a',\n",
       " 'Wilmington',\n",
       " ',',\n",
       " 'Del',\n",
       " '.',\n",
       " ',',\n",
       " 'information',\n",
       " 'service',\n",
       " '.',\n",
       " 'On',\n",
       " 'three-month',\n",
       " '\"',\n",
       " 'consumer',\n",
       " '\"',\n",
       " 'deposits',\n",
       " ',',\n",
       " 'the',\n",
       " 'average',\n",
       " 'yield',\n",
       " 'sank',\n",
       " 'to',\n",
       " '5.29',\n",
       " '%',\n",
       " 'from',\n",
       " '5.42',\n",
       " '%',\n",
       " 'the',\n",
       " 'week',\n",
       " 'before',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'Banxquote',\n",
       " '.',\n",
       " 'Two',\n",
       " 'banks',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Banxquote',\n",
       " 'survey',\n",
       " ',',\n",
       " 'Citibank',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'and',\n",
       " 'CoreStates',\n",
       " 'in',\n",
       " 'Pennsylvania',\n",
       " ',',\n",
       " 'are',\n",
       " 'paying',\n",
       " 'less',\n",
       " 'than',\n",
       " '5',\n",
       " '%',\n",
       " 'on',\n",
       " 'threemonth',\n",
       " 'small-denomination',\n",
       " 'CDs',\n",
       " '.',\n",
       " 'Declines',\n",
       " 'were',\n",
       " 'somewhat',\n",
       " 'smaller',\n",
       " 'on',\n",
       " 'five-year',\n",
       " 'consumer',\n",
       " 'CDs',\n",
       " ',',\n",
       " 'which',\n",
       " 'eased',\n",
       " 'to',\n",
       " '7.37',\n",
       " '%',\n",
       " 'from',\n",
       " '7.45',\n",
       " '%',\n",
       " ',',\n",
       " 'Banxquote',\n",
       " 'said',\n",
       " '.',\n",
       " 'Yields',\n",
       " 'on',\n",
       " 'three-month',\n",
       " 'and',\n",
       " 'six-month',\n",
       " 'Treasury',\n",
       " 'bills',\n",
       " 'sold',\n",
       " 'at',\n",
       " \"Monday's\",\n",
       " 'auction',\n",
       " 'plummeted',\n",
       " 'more',\n",
       " 'than',\n",
       " 'a',\n",
       " 'fifth',\n",
       " 'of',\n",
       " 'a',\n",
       " 'percentage',\n",
       " 'point',\n",
       " 'from',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'week',\n",
       " ',',\n",
       " 'to',\n",
       " '5.46',\n",
       " '%',\n",
       " 'and',\n",
       " '5.63',\n",
       " '%',\n",
       " ',',\n",
       " 'respectively',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html(TweetTokenizer().tokenize(df_eco.text.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_url_hashtags(corpus, tweets=False):\n",
    "  if tweets:\n",
    "    tokenizer = TweetTokenizer()\n",
    "  else:\n",
    "    tokenizer = TreebankWordTokenizer()  \n",
    "    # Life hack : treebank word tokenizer won't keep html code in one token.\n",
    "    # To preprocess economics news corpus, use tweettokenizer. \n",
    "  tokenized_sentences = []\n",
    "  for sample in tqdm(corpus):\n",
    "    # separating sentences\n",
    "    for sentence in sent_detector.tokenize(sample):\n",
    "      tokens = tokenizer.tokenize(sentence)\n",
    "      tokens = remove_url(tokens)\n",
    "      tokens = remove_html(tokens)\n",
    "      tokens = remove_hashtags(tokens)\n",
    "      tokens = list(map(lambda x: x.lower(), tokens))\n",
    "      tokenized_sentences.append(tokens)\n",
    "  return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1579.35it/s]\n",
      "100%|██████████| 8000/8000 [00:27<00:00, 289.06it/s]\n",
      "100%|██████████| 44512/44512 [00:38<00:00, 1159.49it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_pol = tokenize_url_hashtags(df_pol.text.array, tweets=True)\n",
    "cleaned_eco = tokenize_url_hashtags(df_eco.text.array, tweets=False)\n",
    "cleaned_movie = tokenize_url_hashtags(df_movies.overview.dropna().array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we suggest using the Phrases module to train the multi-word detector then iterating on the output to display the results (first 15 for instance)\n",
    "def show_phrases(tokenized_sentences, threshold=10, shown=15):\n",
    "  # Training the multi-word expression detector\n",
    "  phrases = Phrases(tokenized_sentences, threshold=threshold)\n",
    "  i = 0\n",
    "  for phrase, score in phrases.export_phrases(tokenized_sentences):\n",
    "    if i>shown:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Expression : {0}, score = {1}\".format(phrase, score))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus, threshold=50, tweets=False):\n",
    "  tokenized_sentences = tokenize_url_hashtags(corpus, tweets=tweets)\n",
    "  phrases = Phrases(tokenized_sentences, threshold=threshold)\n",
    "\n",
    "  # This lets you use it with less RAM and faster processing.\n",
    "  # But it will no longer be possible to update the detector with new training \n",
    "  # samples\n",
    "  phraser = Phraser(phrases)\n",
    "\n",
    "  # Merging multi-word expressions in the tokenization\n",
    "  clean_corpus = []\n",
    "  for sentence in tokenized_sentences:\n",
    "    clean_corpus.append(phraser[sentence])\n",
    "  \n",
    "  return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1544.01it/s]\n",
      "100%|██████████| 8000/8000 [00:26<00:00, 306.83it/s]\n",
      "100%|██████████| 44512/44512 [00:38<00:00, 1146.55it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_pol = clean_corpus(df_pol.text.array, threshold = 1000, tweets=True)\n",
    "cleaned_eco = clean_corpus(df_eco.text.array,  threshold = 100, tweets=False)\n",
    "cleaned_movie = clean_corpus(df_movies.overview.dropna().array,  threshold = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['april', 'is', 'sexual_assault', 'awareness', 'month', '.']\n",
      "['new_york', '--', 'yields', 'on', 'most', 'certificates', 'of', 'deposit', 'offered', 'by', 'major', 'banks', 'dropped', 'more', 'than', 'a', 'tenth', 'of', 'a', 'percentage_point', 'in', 'the', 'latest', 'week', ',', 'reflecting', 'the', 'overall', 'decline', 'in', 'short-term', 'interest_rates.', '<', '/br', '>', '<', '/br', '>', 'on', 'small-denomination', ',', 'or', '``', 'consumer', ',', \"''\", 'cds', 'sold', 'directly', 'by', 'banks', ',', 'the', 'average', 'yield', 'on', 'six-month', 'deposits', 'fell', 'to', '5.49', '%', 'from', '5.62', '%', 'in', 'the', 'week', 'ended', 'yesterday', ',', 'according', 'to', 'an', '18-bank', 'survey', 'by', 'banxquote', 'money', 'markets', ',', 'a', 'wilmington', ',', 'del.', ',', 'information', 'service.', '<', '/br', '>', '<', '/br', '>', 'on', 'three-month', '``', 'consumer', \"''\", 'deposits', ',', 'the', 'average', 'yield', 'sank', 'to', '5.29', '%', 'from', '5.42', '%', 'the', 'week', 'before', ',', 'according', 'to', 'banxquote', '.']\n",
      "['also', 'starring', 'kate', 'bosworth', ',', 'danny', 'huston', ',', 'tony', 'cox', 'and', 'academy_award', 'winner', 'geoffrey', 'rush', '.']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_pol[184])\n",
    "print(cleaned_eco[0])\n",
    "print(cleaned_movie[42723])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The virtual instance has 8 cpus, that will be used to train the word2vec model\n",
      "Training the political W2V ...\n",
      "Training the economics W2V ...\n",
      "Training the movies' W2V ...\n"
     ]
    }
   ],
   "source": [
    "cpu = cpu_count()\n",
    "print('The virtual instance has {} cpus, that will be used to train the word2vec model'.format(cpu))\n",
    "\n",
    "# We will just get the \"WordVectors\" parameter from the trained Word2Vec model.\n",
    "# Otherwise, we could continue training with some more exemples that could be\n",
    "# fed on the fly to the model.\n",
    "print(\"Training the political W2V ...\")\n",
    "pol = Word2Vec(cleaned_pol, size=100, window=5, min_count=3, workers=cpu)\n",
    "pol.train(cleaned_pol, total_examples=len(cleaned_pol), epochs=10)\n",
    "pol_wv = pol.wv\n",
    "print(\"Training the economics W2V ...\")\n",
    "eco = Word2Vec(cleaned_eco, size=100, window=5, min_count=3, workers=cpu)\n",
    "eco.train(cleaned_eco, total_examples=len(cleaned_eco), epochs=10)\n",
    "eco_wv = eco.wv\n",
    "print(\"Training the movies' W2V ...\")\n",
    "mo = Word2Vec(cleaned_movie, size=100, window=5, min_count=3, workers=cpu)\n",
    "mo.train(cleaned_movie, total_examples=len(cleaned_movie), epochs=10)\n",
    "mo_wv = mo.wv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
